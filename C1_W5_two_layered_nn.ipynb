{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Layered Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initialize parameters function for the 2-layer neural network\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Initializes the weights and biases for a 2-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    n_x -- size of the input layer (number of features)     ; if image is 24 x 24 x 3, n_x = 24*24*3 = 1728\n",
    "    n_h -- size of the hidden layer                         ; if input layer is 7 then n_h = 7\n",
    "    n_y -- size of the output layer (number of classes)     ; if output layer is 1 then n_y = 1\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing initialized weights (W1, W2) and biases (b1, b2)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01  # Weight matrix for first layer\n",
    "    b1 = np.zeros((n_h, 1))  # Bias vector for first layer\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01  # Weight matrix for second layer\n",
    "    b2 = np.zeros((n_y, 1))  # Bias vector for second layer\n",
    "\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Linear forward function for each layer\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a single layer, including the activation function.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from the previous layer (or input data)\n",
    "    W -- weight matrix of the current layer\n",
    "    b -- bias vector of the current layer\n",
    "    activation -- activation function to be used (\"sigmoid\" or \"relu\")\n",
    "\n",
    "    Returns:\n",
    "    A -- activation of the current layer\n",
    "    cache -- tuple containing useful values for backward propagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b  # Linear forward step\n",
    "\n",
    "    # Apply activation function\n",
    "    if activation == \"sigmoid\":\n",
    "        A = 1 / (1 + np.exp(-Z))  # Sigmoid activation\n",
    "    elif activation == \"relu\":\n",
    "        A = np.maximum(0, Z)  # ReLU activation\n",
    "\n",
    "    cache = (A_prev, W, b, Z)  # Cache values for backpropagation\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "# Compute the cost function (binary cross-entropy)\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost (loss) for the neural network using cross-entropy.\n",
    "\n",
    "    Arguments:\n",
    "    A2 -- predicted output from the network (final layer activation)\n",
    "    Y -- true labels (1 if cat, 0 if non-cat)\n",
    "\n",
    "    Returns:\n",
    "    cost -- the cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]  # Number of examples\n",
    "    cost = -np.sum(Y * np.log(A2), (1 - Y) * np.log(1 - A2)) / m\n",
    "    cost = np.squeeze(cost)  # Ensure cost is a scalar value\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Backward propagation function for a single layer\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a single layer, including the activation function.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the activation of the current layer\n",
    "    cache -- tuple containing values from forward propagation (A_prev, W, b, Z)\n",
    "    activation -- the activation function used (\"sigmoid\" or \"relu\")\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the activation of the previous layer\n",
    "    dW -- gradient of the cost with respect to the weights of the current layer\n",
    "    db -- gradient of the cost with respect to the bias of the current layer\n",
    "    \"\"\"\n",
    "    A_prev, W, b, Z = cache\n",
    "    m = A_prev.shape[1]  # Number of examples\n",
    "\n",
    "    # Compute gradients based on the activation function used\n",
    "    if activation == \"relu\":\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0  # Derivative of ReLU\n",
    "    elif activation == \"sigmoid\":\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        dZ = dA * s * (1 - s)  # Derivative of sigmoid\n",
    "\n",
    "    # Calculate gradients\n",
    "    dW = np.dot(dZ, A_prev.T) / m  # Gradient for weights\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m  # Gradient for biases\n",
    "    dA_prev = np.dot(W.T, dZ)  # Gradient for the previous layer's activations\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "# Function for updating parameters (gradient descent)\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing current weights and biases\n",
    "    grads -- dictionary containing gradients (dW1, db1, dW2, db2)\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "\n",
    "    Returns:\n",
    "    parameters -- updated parameters after gradient descent\n",
    "    \"\"\"\n",
    "    parameters[\"W1\"] -= learning_rate * grads[\"dW1\"]  # Update weights of layer 1\n",
    "    parameters[\"b1\"] -= learning_rate * grads[\"db1\"]  # Update biases of layer 1\n",
    "    parameters[\"W2\"] -= learning_rate * grads[\"dW2\"]  # Update weights of layer 2\n",
    "    parameters[\"b2\"] -= learning_rate * grads[\"db2\"]  # Update biases of layer 2\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Two-layer neural network model\n",
    "def two_layer_model(\n",
    "    X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (n_x, number of examples)\n",
    "    Y -- true labels, shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    num_iterations -- number of iterations for training\n",
    "    print_cost -- if True, prints the cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing the updated parameters\n",
    "    costs -- list of costs over training\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    grads = {}  # Dictionary to store gradients\n",
    "    costs = []  # List to store cost values\n",
    "    m = X.shape[1]  # Number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims  # Unpack layer dimensions\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1, b1, W2, b2 = (\n",
    "        parameters[\"W1\"],\n",
    "        parameters[\"b1\"],\n",
    "        parameters[\"W2\"],\n",
    "        parameters[\"b2\"],\n",
    "    )\n",
    "\n",
    "    # Gradient descent loop\n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "\n",
    "        # Compute the cost (loss)\n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        dA2 = -(\n",
    "            np.divide(Y, A2) - np.divide(1 - Y, 1 - A2)\n",
    "        )  # Gradient of the cost wrt A2\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "\n",
    "        # Store gradients\n",
    "        grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"] = dW1, db1, dW2, db2\n",
    "\n",
    "        # Update parameters using gradient descent\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Retrieve updated parameters for the next iteration\n",
    "        W1, b1, W2, b2 = (\n",
    "            parameters[\"W1\"],\n",
    "            parameters[\"b1\"],\n",
    "            parameters[\"W2\"],\n",
    "            parameters[\"b2\"],\n",
    "        )\n",
    "\n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Layered Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize parameters for L-layer deep neural network\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes the weights and biases for an L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    layer_dims -- list containing the dimensions of each layer in the network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing initialized weights and biases for each layer\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # Number of layers in the network (including input layer)\n",
    "\n",
    "    # Initialize weights and biases for each layer l (from 1 to L-1)\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = (\n",
    "            np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        )  # Weight matrix\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))  # Bias vector\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Linear forward function for each layer\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a single layer, including the activation function.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from the previous layer (or input data)\n",
    "    W -- weight matrix of the current layer\n",
    "    b -- bias vector of the current layer\n",
    "    activation -- activation function to be used (\"sigmoid\" or \"relu\")\n",
    "\n",
    "    Returns:\n",
    "    A -- activation of the current layer\n",
    "    cache -- tuple containing useful values for backward propagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b  # Linear forward step\n",
    "\n",
    "    # Apply activation function\n",
    "    if activation == \"sigmoid\":\n",
    "        A = 1 / (1 + np.exp(-Z))  # Sigmoid activation\n",
    "    elif activation == \"relu\":\n",
    "        A = np.maximum(0, Z)  # ReLU activation\n",
    "\n",
    "    cache = (A_prev, W, b, Z)  # Cache values for backpropagation\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "# Forward propagation for L-layer neural network\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for the L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data of shape (n_x, number of examples)\n",
    "    parameters -- dictionary containing the weights and biases for all layers\n",
    "\n",
    "    Returns:\n",
    "    AL -- output of the last (Lth) layer (activation of the final layer)\n",
    "    caches -- list of caches containing every cache of linear_activation_forward() (used for backpropagation)\n",
    "    \"\"\"\n",
    "    caches = []  # To store intermediate values for backpropagation\n",
    "    A = X  # Initialize activation as the input data\n",
    "    L = len(parameters) // 2  # Number of layers (W1, W2, ..., WL)\n",
    "\n",
    "    # Implement forward propagation for layers 1 to L-1 (LINEAR -> RELU)\n",
    "    for l in range(1, L):\n",
    "        A_prev = A  # Activation from previous layer\n",
    "        A, cache = linear_activation_forward(\n",
    "            A_prev,\n",
    "            parameters[\"W\" + str(l)],\n",
    "            parameters[\"b\" + str(l)],\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        caches.append(cache)  # Store cache for backpropagation\n",
    "\n",
    "    # Implement forward propagation for the last layer L (LINEAR -> SIGMOID)\n",
    "    AL, cache = linear_activation_forward(\n",
    "        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation=\"sigmoid\"\n",
    "    )\n",
    "    caches.append(cache)  # Store cache for backpropagation\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "\n",
    "# Backward propagation function for a single layer\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a single layer, including the activation function.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the activation of the current layer\n",
    "    cache -- tuple containing values from forward propagation (A_prev, W, b, Z)\n",
    "    activation -- the activation function used (\"sigmoid\" or \"relu\")\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the activation of the previous layer\n",
    "    dW -- gradient of the cost with respect to the weights of the current layer\n",
    "    db -- gradient of the cost with respect to the bias of the current layer\n",
    "    \"\"\"\n",
    "    A_prev, W, b, Z = cache\n",
    "    m = A_prev.shape[1]  # Number of examples\n",
    "\n",
    "    # Compute gradients based on the activation function used\n",
    "    if activation == \"relu\":\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0  # Derivative of ReLU\n",
    "    elif activation == \"sigmoid\":\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        dZ = dA * s * (1 - s)  # Derivative of sigmoid\n",
    "\n",
    "    # Calculate gradients\n",
    "    dW = np.dot(dZ, A_prev.T) / m  # Gradient for weights\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m  # Gradient for biases\n",
    "    dA_prev = np.dot(W.T, dZ)  # Gradient for the previous layer's activations\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "# Compute cost function (binary cross-entropy)\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function (binary cross-entropy) for the L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to the predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (1 for cat, 0 for non-cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- binary cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]  # Number of examples\n",
    "\n",
    "    # Compute binary cross-entropy cost\n",
    "    cost = -np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)) / m\n",
    "    cost = np.squeeze(cost)  # Ensures cost is a scalar value\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Backward propagation for L-layer neural network\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for the L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector (predicted output from forward propagation)\n",
    "    Y -- true \"label\" vector\n",
    "    caches -- list of caches containing every cache of forward propagation\n",
    "\n",
    "    Returns:\n",
    "    grads -- dictionary with gradients for every layer\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)  # Number of layers\n",
    "    m = AL.shape[1]  # Number of examples\n",
    "    Y = Y.reshape(AL.shape)  # Ensure Y has the same shape as AL\n",
    "\n",
    "    # Initialize the backpropagation for the last layer (Lth layer: SIGMOID -> LINEAR)\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))  # Derivative of cost w.r.t AL\n",
    "    current_cache = caches[L - 1]  # Cache from last layer\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = (\n",
    "        linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    )\n",
    "\n",
    "    # Loop backward from L-2 to 0 for layers (RELU -> LINEAR)\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]  # Cache from layer l\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
    "            grads[\"dA\" + str(l + 1)], current_cache, \"relu\"\n",
    "        )\n",
    "        grads[\"dA\" + str(l)] = (\n",
    "            dA_prev_temp  # Gradient for the previous layer's activation\n",
    "        )\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp  # Gradient for the current layer's weights\n",
    "        grads[\"db\" + str(l + 1)] = db_temp  # Gradient for the current layer's biases\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# Update parameters for L-layer neural network\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent for the L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing the current weights and biases\n",
    "    grads -- dictionary containing gradients for the weights and biases\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "\n",
    "    Returns:\n",
    "    parameters -- updated parameters after gradient descent\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # Number of layers in the neural network\n",
    "\n",
    "    # Update parameters for each layer l\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] -= (\n",
    "            learning_rate * grads[\"dW\" + str(l)]\n",
    "        )  # Update weights\n",
    "        parameters[\"b\" + str(l)] -= (\n",
    "            learning_rate * grads[\"db\" + str(l)]\n",
    "        )  # Update biases\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# L-layer neural network model\n",
    "def L_layer_model(\n",
    "    X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements an L-layer neural network: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector, shape (1, number of examples)\n",
    "    layers_dims -- list containing the dimensions of each layer\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    num_iterations -- number of iterations for training the model\n",
    "    print_cost -- if True, print the cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model\n",
    "    costs -- list of costs after every 100 iterations (for plotting the learning curve)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []  # List to store the cost after every 100 iterations\n",
    "\n",
    "    # Initialize parameters for the L-layer model\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # Gradient descent loop for num_iterations\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
